{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the prediction process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "First, let's load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "np.random.seed(40)\n",
    "\n",
    "data = pd.read_csv(\"churn.csv\",sep=',',index_col=0)\n",
    "\n",
    "y = data['Churn']\n",
    "X = data.drop('Churn',axis=1)\n",
    "\n",
    "#print(data.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "def convert_and_remove_categorical_variables(X, to_convert, to_remove):\n",
    "    for variable in X.columns:    \n",
    "        if variable in to_convert:\n",
    "            if len(X[variable].unique()) < 10:\n",
    "                X = pd.concat([X,pd.get_dummies(X[variable], prefix=variable, drop_first=True)],axis=1).drop([variable],axis=1)  \n",
    "        elif variable in to_remove:\n",
    "            X = X.drop([variable],axis=1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "to_convert = ['Area_Code','International_Plan','Voice_mail_Plan']\n",
    "to_remove = ['Phone_Number']\n",
    "X = data.drop('Churn',axis=1)\n",
    "X = convert_and_remove_categorical_variables(X, to_convert, to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()    \n",
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit, KFold, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.tree import DecisionTreeClassifier as DT\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The evaluation setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Let's now prepare our evaluation setup. To do this, you will implement a 10-Fold cross-validation, as well as a stratified and shuffled 10-Fold CV. \n",
    "\n",
    "I have prepared three models which you have to test. Make sure you also normalize your data first. Calculate the mean of the three metrics (the mean of their mean results over the 5-fold CV), and return the best-performing model. After obtaining the best model (classifier), it is fit to the training data; the function should return the model fitted with the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, write a function that applies the best model you have found during cross-validation with your final test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_process(X_train, y_train, n_folds, shuffled): \n",
    "    \n",
    "    from sklearn.model_selection import ShuffleSplit, KFold, StratifiedKFold\n",
    "    from sklearn.linear_model import LogisticRegression as LR\n",
    "    from sklearn.tree import DecisionTreeClassifier as DT\n",
    "    from sklearn.ensemble import RandomForestClassifier as RF\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.preprocessing import Normalizer\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "    from sklearn.model_selection import cross_validate\n",
    "    from sklearn.model_selection import train_test_split as tts\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    #import warnings\n",
    "    #warnings.filterwarnings(\"ignore\")\n",
    "    overall_metric = []\n",
    "    overall_precision = []\n",
    "    overall_accuracy = []\n",
    "    overall_auroc = []\n",
    "    metric = []\n",
    "    # Your output should be a cross_validate() object\n",
    "    np.random.seed(42)\n",
    "    models = [LR(solver = 'liblinear'), DT(), RF(n_estimators = 10, random_state = 99)]\n",
    "    #best_model = models[0]\n",
    "    k_fold = KFold(n_splits = n_folds, random_state = 42)\n",
    "    stratified_kfold = StratifiedKFold(n_splits= n_folds, random_state = 42)\n",
    "    shuffled_stratified = ShuffleSplit(n_folds, test_size = 0.3, random_state = 42)\n",
    "    CV_type = [k_fold, stratified_kfold, shuffled_stratified]\n",
    "    for i in models:\n",
    "        metric_2 = 0\n",
    "        precision = 0\n",
    "        accuracy = 0\n",
    "        auroc = 0\n",
    "        pipeline = make_pipeline(StandardScaler(), i)\n",
    "        for j in CV_type:\n",
    "\n",
    "            \n",
    "\n",
    "            # metrics you want to have computed\n",
    "            metrics = ['accuracy','precision','roc_auc']\n",
    "            metric = ['test_accuracy','test_precision', 'test_roc_auc']\n",
    "            # By default, we should not really care about the training scores. To show them, we add the extra return_train_score parameter\n",
    "            outcomes = cross_validate(pipeline, X_train, y_train, scoring=metrics, cv=j, return_train_score=False)\n",
    "            \n",
    "            accuracy_1 = np.average(outcomes['test_accuracy'])\n",
    "            precision_1 = np.average(outcomes['test_precision'])\n",
    "            roc_auc_1 = np.average(outcomes['test_roc_auc'])\n",
    "            accuracy += accuracy_1\n",
    "            precision += precision_1\n",
    "            auroc += roc_auc_1\n",
    "                \n",
    "        overall_accuracy.append(accuracy / 3)\n",
    "        overall_precision.append(precision / 3)\n",
    "        overall_auroc.append(auroc / 3)\n",
    "        \n",
    "    #print(overall_accuracy)\n",
    "    #print(overall_precision)\n",
    "    #print(overall_auroc)\n",
    "    for i in range(len(models)):\n",
    "        average_metric = (overall_accuracy[i] + overall_precision[i] + overall_auroc[i]) / 3\n",
    "        overall_metric.append(average_metric)\n",
    "    #print(overall_metric)\n",
    "    sorted = np.argsort(overall_metric)[::-1]\n",
    "    best_classifier = models[sorted[0]]\n",
    "    #print(best_classifier)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    best_model = best_classifier.fit(X_train_scaled, y_train)\n",
    "    #overall_metric = overall_metric / 9\n",
    "    return best_model \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_evaluation_test_set(model, X_test, y_test):\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    accuracy = 0\n",
    "    precision = 0\n",
    "    auroc = 0\n",
    "    y_predict = model.predict(X_test_scaled)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    precision = precision_score(y_test, y_predict)\n",
    "    auroc = roc_auc_score(y_test, y_predict)\n",
    "    ###\n",
    "    ### YOUR CODE HERE\n",
    "    ###\n",
    "    \n",
    "    return accuracy, precision, auroc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Now, verify your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "check_evaluation",
     "locked": true,
     "points": "15",
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "np.random.seed(42)\n",
    "X_train, X_test, y_train, y_test = tts(X, y, test_size = 0.3)\n",
    "\n",
    "best_model = evaluation_process(X_train, y_train, 5, True)\n",
    "assert np.allclose(get_evaluation_test_set(best_model,X_test, y_test), (0.9506666666666667, 0.9290322580645162, 0.8435724133292982), rtol=0.05)\n",
    "\n",
    "\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "So now you have implemented lots of models, used the training set to find the best model, and eventually made your final evaluation based on the test set, after which you should not build any more models. The reason for this is that once you use data for creating a model, you will always have obtained knowledge about the data. Hence, there is always a leak if you make decisions to further alter the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 [3.6]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
